{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8668/8668 [00:00<00:00, 299105.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_scene: 3959\n",
      "incorrect_scene: 397\n",
      "Total: 4356\n",
      "Scene Classify Accuracy: 0.9088613406795225\n",
      "New Average Precision: 0.8428\n",
      "New Average Recall: 0.8164\n",
      "New F1 score: 0.8294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8668/8668 [00:00<00:00, 580501.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_scene: 3886\n",
      "incorrect_scene: 470\n",
      "Total: 4356\n",
      "Scene Classify Accuracy: 0.8921028466483012\n",
      "New Average Precision: 0.8635\n",
      "New Average Recall: 0.7785\n",
      "New F1 score: 0.8188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "def calculate_precision_recall(gt_class, pred_class):\n",
    "    gt_rels = set(gt_class)\n",
    "    pred_rels = set(pred_class)\n",
    "    # Calculate the number of true positives (tp), false positives (fp), and false negatives (fn)\n",
    "    tp = len(gt_rels & pred_rels)\n",
    "    fp = len(pred_rels - gt_rels)\n",
    "    fn = len(gt_rels - pred_rels)\n",
    "    # Calculate precision and recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_tpfpfn(gt_class, pred_class):\n",
    "    gt_rels = set(gt_class)\n",
    "    pred_rels = set(pred_class)\n",
    "    # Calculate the number of true positives (tp), false positives (fp), and false negatives (fn)\n",
    "    tp = len(gt_rels & pred_rels)\n",
    "    fp = len(pred_rels - gt_rels)\n",
    "    fn = len(gt_rels - pred_rels)\n",
    "    return tp, fp, fn\n",
    "\n",
    "def calculate_PRF1(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return precision, recall, f1\n",
    "\n",
    "def evaluation_metrics(data_path):\n",
    "\n",
    "    base = [json.loads(q) for q in open(data_path, \"r\")]\n",
    "    correct_single=0\n",
    "    incorrect_single=0\n",
    "    count = 0\n",
    "    tp_total = 0\n",
    "    fp_total = 0\n",
    "    fn_total = 0\n",
    "    for answers in tqdm(base):\n",
    "        question_text = answers['prompt']\n",
    "        if question_text.endswith(\"Answer in one word or a short phrase.\"):\n",
    "            mode = \"single\"\n",
    "        elif question_text.endswith(\"Answer with all applicable classes separated by commas.\"):\n",
    "            mode = \"multi\"\n",
    "        \n",
    "        gt=answers['ground_truth'].lower()\n",
    "        if mode == \"single\":\n",
    "            if gt==answers['text'].lower():\n",
    "                correct_single=correct_single+1\n",
    "            else:\n",
    "                incorrect_single=incorrect_single+1\n",
    "\n",
    "        elif mode == \"multi\":\n",
    "            gt_obj = [label.strip() for label in gt.split(\",\")]\n",
    "            answer_obj = [an.strip() for an in answers['text'].lower().split(\",\")]\n",
    "            tp, fp, fn = calculate_tpfpfn(gt_obj, answer_obj)\n",
    "            tp_total+=tp\n",
    "            fp_total+=fp\n",
    "            fn_total+=fn\n",
    "            count += 1\n",
    "            \n",
    "    print('correct_scene:',correct_single)\n",
    "    print('incorrect_scene:',incorrect_single)\n",
    "    print('Total:',correct_single+incorrect_single)\n",
    "    if (correct_single+incorrect_single)>0:\n",
    "        print('Scene Classify Accuracy:',(correct_single/(correct_single+incorrect_single)))\n",
    "\n",
    "    precision_total, recall_total, f1_total = calculate_PRF1(tp_total, fp_total, fn_total)\n",
    "    print(f'New Average Precision: {precision_total:.4f}')\n",
    "    print(f'New Average Recall: {recall_total:.4f}')\n",
    "    print(f'New F1 score: {f1_total:.4f}')\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/imageclassify/vila_train_sisv_sftV2.jsonl\")\n",
    "'''\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/imageclassify/vila_train_sisv_sftV3.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/imageclassify/vila_train_sisv_pretrain_35wV2.jsonl\")\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/imageclassify/vila_train_sisv_pretrain_35w.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/imageclassify/vila_train_sisv_pretrain_35w_att.jsonl\")\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/imageclassify/vila_train_sisv_pretrain_35w_attV2.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/imageclassify/vila_train_sisv_pretrain_35w_clean.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/imageclassify/vila_train_sisv_pretrain_att_35w_clean.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/imageclassify/vila_train_sisv_pretrain_att_100w_clean.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/imageclassify/vila_train_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/imageclassify/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/imageclassify/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/imageclassify/vila_train_all_sisv_pretrain_att_186w_clean_3b.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/imageclassify/vila_train_all_sisv_clean_8b.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/imageclassify/vila_train_all_sisv_pretrain_186w_clean_only_si.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/imageclassify/vila_train_all_sisv_pretrain_186w_clean_only_sv.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/imageclassify/vila_train_all_sisv_pretrain_186w_clean.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/imageclassify/vila_train_all_sisv_att_186w_clean_13bV2.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/last_results/imageclassify/vila_train_all_sisv_pretrain_att_186w_clean_8BV3.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/rebuttal_results/imageclassify/vila_train_all_sisv_pretrain_att_100w_clean_8b.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/imageclassify/vila_train_all_sisv_pretrain_att_186w_clean_3b.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/imageclassify/vila_train_all_sisv_att_186w_clean_13bV2.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/imageclassify/vila_train_all_sisv_pretrain_att_35w_clean_8bV2.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/imageclassify/vila_train_all_sisv_pretrain_att_35w_clean_8bV4.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/imageclassify/vila_train_all_sisv_pretrain_att_100w_clean_8b.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/imageclassify/Llama-3-VILA1.5-8B.jsonl\")\n",
    "vila_train_all_sisv_pretrain_att_186w_clean_8B_llava_instruct150k\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/benchmark_results/aid/vila_train_all_clean_8B_vqa.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/benchmark_results/UCMerced_LandUse/vila_train_all_clean_8B_vqa.jsonl\")\n",
    "'''\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/last_results/imageclassify/vila_train_all_sisv_pretrain_att_186w_clean_8B_llava_instruct150k.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/last_results/imageclassify/vila_train_all_sisv_pretrain_att_sharegpt4v_186w_clean_8B_llava_instruct150k.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4646/4646 [00:00<00:00, 955935.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 4355\n",
      "incorrect: 291\n",
      "Total: 4646\n",
      "Acc: 0.9373654756780025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4646/4646 [00:00<00:00, 1379689.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 4386\n",
      "incorrect: 260\n",
      "Total: 4646\n",
      "Acc: 0.9440378820490745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#分类通用\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "def evaluation_metrics(data_path):\n",
    "        \n",
    "    base = [json.loads(q) for q in open(data_path, \"r\")]#[:1000]\n",
    "    correct=0\n",
    "    incorrect=0\n",
    "    for answers in tqdm(base):\n",
    "        gt=answers['ground_truth'].split('/')[0].lower().replace('.','')\n",
    "        answer=answers['text'].replace(' ','').lower().replace('.','')\n",
    "        if gt==answer:\n",
    "                correct=correct+1\n",
    "        else:\n",
    "                incorrect=incorrect+1\n",
    "        # else:\n",
    "        #     continue\n",
    "    print('correct:',correct)\n",
    "    print('incorrect:',incorrect)\n",
    "    print('Total:',correct+incorrect)\n",
    "    print('Acc:',(correct/(correct+incorrect)))\n",
    "'''   \n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sample_pretrain_add_sft.jsonl\")\n",
    "  \n",
    "evaluation_metrics('/mnt/public/vila-1.5/our_results/citylocation/vila_train_sample_pretrain_att_sft.jsonl')\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sample_pretrain_att_add_sft.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sample_pretrain_att_0915_sft.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sample_pretrain_0916_sft.jsonl\")\n",
    "'''\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sisv_sftV2.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sisv_sftV3.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sisv_pretrain_35w.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sisv_pretrain_35wV2.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sisv_pretrain_35w_att.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sisv_pretrain_35w_attV2.jsonl\")\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sisv_pretrain_35w_clean.jsonl\")\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sisv_pretrain_att_35w_clean.jsonl\")\n",
    "'''\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sisv_pretrain_att_100w_clean.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/citylocation/vila_train_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/citylocation/vila_train_all_sisv_pretrain_att_186w_clean_3b.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/citylocation/vila_train_all_sisv_clean_8b.jsonl\")\n",
    "\n",
    "\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/sv/CityLoc/vila_train_all_sisv_clean_8b.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/sv/CityLoc/vila_train_all_sisv_pretrain_186w_clean_only_si.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/sv/CityLoc/vila_train_all_sisv_pretrain_186w_clean_only_sv.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/sv/CityLoc/vila_train_all_sisv_pretrain_186w_clean.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/citylocation/vila_train_all_sisv_pretrain_att_186w_clean_13b.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/sv/CityLoc/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/sv/CityLoc/vila_train_all_sisv_pretrain_att_186w_clean_3b.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/citylocation/vila_train_all_sisv_pretrain_att_186w_clean_13b.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/citylocation/vila_train_all_sisv_att_90w_clean_13bV2_our_task.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/UCMerced_LandUse/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/citylocation/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/citylocation/vila_train_all_sisv_pretrain_att_186w_clean_3b.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/citylocation/vila_train_all_sisv_pretrain_att_186w_clean_3b.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/citylocation/vila_train_all_sisv_clean_8b.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/citylocation/vila_train_all_sisv_pretrain_186w_clean_only_si.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/citylocation/vila_train_all_sisv_pretrain_186w_clean_only_sv.jsonl\")\n",
    "\n",
    "\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/sv/CityLoc/vila_train_sisv_sftV2.jsonl\")vila_train_all_sisv_pretrain_186w_clean\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/sv/CityLoc/vila_train_sisv_pretrain_35wV2.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/sv/CityLoc/vila_train_all_sisv_att_186w_clean_13bV2.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/citylocation/vila_train_all_sisv_pretrain_att_35w_clean_8bV2.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/citylocation/vila_train_all_sisv_pretrain_att_100w_clean_8b.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/citylocation/vila_train_all_sisv_pretrain_att_35w_clean_8bV3.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/citylocation/vila_train_all_sisv_pretrain_att_35w_clean_8bV4.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/citylocation/Llama-3-VILA1.5-8B.jsonl\")\n",
    "'''\n",
    "\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/last_results/citylocation/vila_train_all_sisv_pretrain_att_186w_clean_8B_llava_instruct150k.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/last_results/citylocation/vila_train_all_sisv_pretrain_att_sharegpt4v_186w_clean_8B_llava_instruct150k.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 727/727 [00:00<00:00, 840711.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 385\n",
      "incorrect: 342\n",
      "Total: 727\n",
      "Acc: 0.5295735900962861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 727/727 [00:00<00:00, 664471.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 110\n",
      "incorrect: 617\n",
      "Total: 727\n",
      "Acc: 0.15130674002751032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 727/727 [00:00<00:00, 619264.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 126\n",
      "incorrect: 601\n",
      "Total: 727\n",
      "Acc: 0.1733149931224209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 727/727 [00:00<00:00, 102262.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 2\n",
      "incorrect: 725\n",
      "Total: 727\n",
      "Acc: 0.002751031636863824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 727/727 [00:00<00:00, 716798.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 110\n",
      "incorrect: 617\n",
      "Total: 727\n",
      "Acc: 0.15130674002751032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 727/727 [00:00<00:00, 771183.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 285\n",
      "incorrect: 442\n",
      "Total: 727\n",
      "Acc: 0.3920220082530949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 727/727 [00:00<00:00, 772159.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 307\n",
      "incorrect: 420\n",
      "Total: 727\n",
      "Acc: 0.422283356258597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 727/727 [00:00<00:00, 761363.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 362\n",
      "incorrect: 365\n",
      "Total: 727\n",
      "Acc: 0.49793672627235214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:00<00:00, 888749.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 599\n",
      "incorrect: 901\n",
      "Total: 1500\n",
      "Acc: 0.3993333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:00<00:00, 852963.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 512\n",
      "incorrect: 988\n",
      "Total: 1500\n",
      "Acc: 0.3413333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/general_results/realworldqa/Llama-3-VILA1.5-8B.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/general_results/realworldqa/vila_train_all_sisv_pretrain_186w_clean.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/general_results/realworldqa/vila_train_all_sisv_pretrain_att_186w_clean_8B_vqa.jsonl\")\n",
    "\n",
    "\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/general_results/realworldqa/pretrain_att_186w_clean.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/general_results/realworldqa/vila_train_all_sisv_pretrain_186w_clean.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/general_results/realworldqa/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/general_results/realworldqa/vila_train_all_sisv_pretrain_att_186w_clean_8B_llava_instruct150k.jsonl\")\n",
    "\n",
    "\n",
    "#evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/general_results/mmstar/vila_train_all_sisv_pretrain_att_186w_clean_8B_llava_instruct150k.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/general_results/realworldqa/vila_train_all_sisv_pretrain_att_sharegpt4v_186w_clean_8B_llava_instruct150k.jsonl\")\n",
    "\n",
    "\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/general_results/mmstar/vila_train_all_sisv_pretrain_att_sharegpt4v_186w_clean_8B_llava_instruct150k.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/general_results/mmstar/vila_train_all_sisv_pretrain_att_186w_clean_8B_llava_instruct150k.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5556/5556 [00:00<00:00, 943005.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category-wise accuracies:\n",
      "count: 739/1553 (47.59%)\n",
      "compare: 337/378 (89.15%)\n",
      "presence: 1714/1769 (96.89%)\n",
      "relationship: 1748/1856 (94.18%)\n",
      "Overall Acc: 81.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8250/8250 [00:00<00:00, 1072329.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category-wise accuracies:\n",
      "object: 1454/2026 (71.77%)\n",
      "subject: 975/2031 (48.01%)\n",
      "exist: 1497/2000 (74.85%)\n",
      "relationship: 779/2193 (35.52%)\n",
      "Overall Acc: 57.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5556/5556 [00:00<00:00, 2031873.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category-wise accuracies:\n",
      "count: 751/1553 (48.36%)\n",
      "compare: 334/378 (88.36%)\n",
      "presence: 1717/1769 (97.06%)\n",
      "relationship: 1759/1856 (94.77%)\n",
      "Overall Acc: 82.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8250/8250 [00:00<00:00, 2214734.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category-wise accuracies:\n",
      "object: 1533/2026 (75.67%)\n",
      "subject: 1246/2031 (61.35%)\n",
      "exist: 1772/2000 (88.60%)\n",
      "relationship: 969/2193 (44.19%)\n",
      "Overall Acc: 66.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "def evaluation_metrics(data_path):\n",
    "        \n",
    "    base = [json.loads(q) for q in open(data_path, \"r\")]\n",
    "    # collect categories\n",
    "    all_categories = set(answers['category'] for answers in base)\n",
    "    category_correct = {category: 0 for category in all_categories}\n",
    "    category_incorrect = {category: 0 for category in all_categories}\n",
    "\n",
    "    correct=0\n",
    "    incorrect=0\n",
    "    for answers in tqdm(base):\n",
    "        gt=answers['ground_truth'].lower()\n",
    "        answer=answers['text'].lower()\n",
    "        category = answers['category'].lower()\n",
    "        if gt==answer:\n",
    "            correct=correct+1\n",
    "            category_correct[category] += 1\n",
    "        else:\n",
    "            incorrect=incorrect+1\n",
    "            category_incorrect[category] += 1\n",
    "            \n",
    "    overall_correct = 0\n",
    "    overall_total = 0\n",
    "\n",
    "    print(\"\\nCategory-wise accuracies:\")\n",
    "    for cat, cat_corr in category_correct.items():\n",
    "        cat_total_count = cat_corr + category_incorrect[cat]\n",
    "        cat_acc = cat_corr/cat_total_count\n",
    "        \n",
    "        print(f\"{cat}: {cat_corr}/{cat_total_count} ({cat_acc*100:.2f}%)\")\n",
    "        \n",
    "        overall_correct += cat_corr\n",
    "        overall_total += cat_total_count\n",
    "    \n",
    "    overall_acc = overall_correct / overall_total\n",
    "\n",
    "    print(f\"Overall Acc: {overall_acc*100:.2f}%\")\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/vqa/vila_train_sisv_sftV2.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/vqa/vila_train_sisv_sftV3.jsonl\")\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/vqa/vila_train_sisv_pretrain_35wV2.jsonl\")\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/vqa/vila_train_sisv_pretrain_35w_att.jsonl\")\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/vqa/vila_train_sisv_pretrain_35w_attV2.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/vqa/vila_train_sisv_pretrain_35w_clean.jsonl\")\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/vqa/vila_train_sisv_pretrain_att_35w_clean.jsonl\")\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/vqa/vila_train_sisv_pretrain_att_100w_clean.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/vqa/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/vqa/vila_train_all_sisv_pretrain_att_186w_clean_3b.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/vqa/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/spatial/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/vqa/vila_train_all_sisv_pretrain_att_186w_clean_3b.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/spatial/vila_train_all_sisv_pretrain_att_186w_clean_3b.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/vqa/vila_train_all_sisv_clean_8b.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/spatial/vila_train_all_sisv_clean_8b.jsonl\")\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/spatial/vila_train_all_sisv_pretrain_186w_clean_only_si.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/vqa/vila_train_all_sisv_pretrain_186w_clean_only_si.jsonl\")\n",
    "\n",
    "\n",
    "'''\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/vqa/vila_train_all_sisv_clean_8b.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/vqa/vila_train_all_sisv_pretrain_186w_clean_only_si.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/vqa/vila_train_all_sisv_pretrain_186w_clean_only_sv.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/spatial/vila_train_all_sisv_pretrain_186w_clean.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/spatial/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/vqa/vila_train_all_sisv_att_186w_clean_13bV2.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/rebuttal_results/vqa/vila_train_all_sisv_pretrain_att_100w_clean_8b.jsonl\")\n",
    "'''\n",
    "'''\n",
    "\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/spatial/vila_train_all_sisv_pretrain_att_35w_clean_8bV2.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/spatial/vila_train_all_sisv_pretrain_att_100w_clean_8b.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/spatial/vila_train_all_sisv_pretrain_att_35w_clean_8bV4.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/spatial/Llama-3-VILA1.5-8B.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/spatial/vila_train_all_sisv_pretrain_att_186w_clean_8B_llava_instruct150k.jsonl\")\n",
    "\n",
    "#evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/vqa/vila_train_all_sisv_pretrain_att_35w_clean_8bV2.jsonl\")\n",
    "#evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/vqa/vila_train_all_sisv_pretrain_att_35w_clean_8bV3.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/vqa/vila_train_all_sisv_pretrain_att_35w_clean_8bV4.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/vqa/vila_train_all_sisv_pretrain_att_100w_clean_8b.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/vqa/Llama-3-VILA1.5-8B.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/rebuttal_results/vqa/vila_train_all_sisv_pretrain_att_100w_clean_8b.jsonl\")\n",
    "'''\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/last_results/vqa/vila_train_all_sisv_pretrain_att_186w_clean_8B_llava_instruct150k.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/last_results/spatial/vila_train_all_sisv_pretrain_att_186w_clean_8B_llava_instruct150k.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/last_results/vqa/vila_train_all_sisv_pretrain_att_sharegpt4v_186w_clean_8B_llava_instruct150k.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/last_results/spatial/vila_train_all_sisv_pretrain_att_sharegpt4v_186w_clean_8B_llava_instruct150k.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/zhangxin/.local/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 13.626240118577075\n",
      "mae: 2.9282608695652175\n",
      "r2: -0.8824377079043177\n",
      "rmse: 3.691373744092716\n",
      "---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/zhangxin/.local/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 5.9739447024060786\n",
      "mae: 1.7031447868298861\n",
      "r2: 0.3238231481475752\n",
      "rmse: 2.4441654408828546\n",
      "---------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import json\n",
    "import   re\n",
    "def compute_accuracy_regression(pred_list, true_list):\n",
    "    mse = metrics.mean_squared_error(true_list, pred_list)\n",
    "    mae = metrics.mean_absolute_error(true_list, pred_list)\n",
    "    r2 = metrics.r2_score(true_list, pred_list)\n",
    "    \n",
    "    rmse = metrics.mean_squared_error(true_list, pred_list, squared=False)\n",
    "    return mse, mae, r2,rmse\n",
    "\n",
    "def evaluation_metrics(data_path):\n",
    "    data = []\n",
    "    with open(data_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pred_list = [] \n",
    "    true_list = [] \n",
    "\n",
    "    for item in data:\n",
    "        sdg_str = item['text']\n",
    "        #print(coordinates_str)\n",
    "        # 去除字符串中的括号并按逗号分割，然后转换为浮点数\n",
    "        #只匹配text中的经纬度\n",
    "        match = re.search(r\"(\\d+\\.\\d+)\", sdg_str)\n",
    "        if match:\n",
    "            \n",
    "            rating = float(match.group(0))\n",
    "            if rating!=0.1:\n",
    "                pred_list.append(rating)  # 将元组添加到列表中\n",
    "            \n",
    "                sdg_str = str(item['ground_truth'])\n",
    "                match = re.search(r\"(\\d+\\.\\d+)\", sdg_str)\n",
    "                if match:\n",
    "                    \n",
    "                    rating = float(match.group(0))\n",
    "                    true_list.append(rating)  # 将元组添加到列表中\n",
    "            \n",
    "    \n",
    "    mse, mae, r2,rmse = compute_accuracy_regression(pred_list, true_list)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.hist(pred_list,bins=100)\n",
    "    plt.hist(true_list,bins=100)\n",
    "    print('mse:',mse)\n",
    "    print('mae:',mae)\n",
    "    print('r2:',r2)\n",
    "    print('rmse:',rmse)\n",
    "    \n",
    "    print(\"---------------\")\n",
    "\n",
    "\n",
    "json_file_path = '/mnt/public/vila-1.5/our_results/pop/vila_train_sample_pretrain_att_sft.jsonl'\n",
    "'''\n",
    "evaluation_metrics(json_file_path)\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sample_pretrain_att_0915_sft.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sample_pretrain_add_sft.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sample_pretrain_att_add_sft.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sample_pretrain_0916_sft.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_pretrain_35w_att.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_pretrain_35w.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_pretrain_35wV2.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_sftV2.jsonl\")\n",
    "'''\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/sv/PlacePulse_more_beautiful/vila_train_sisv_sftV2.jsonl\")\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_sftV2.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_sftV3.jsonl\")\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_pretrain_35w.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_pretrain_35wV2.jsonl\")\n",
    "\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_pretrain_35w_att.jsonl\")\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_pretrain_35w_attV2.jsonl\")\n",
    "'''\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_pretrain_35w_clean.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_pretrain_att_35w_clean.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_pretrain_att_100w_clean.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/our_results/pop/vila_train_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "\n",
    "\n",
    "#evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/pop/vila_train_all_sisv_pretrain_att_186w_clean_3b.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/pop/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/nightlight/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/pop/vila_train_all_sisv_pretrain_att_186w_clean_3b.jsonl\")\n",
    "\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/nightlight/vila_train_all_sisv_pretrain_att_186w_clean_3b.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/nightlight/vila_train_all_sisv_clean_8b.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/pop/vila_train_all_sisv_clean_8b.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/nightlight/vila_train_all_sisv_pretrain_186w_clean_only_si.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/pop/vila_train_all_sisv_pretrain_186w_clean_only_si.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/nightlight/vila_train_all_sisv_pretrain_att_186w_clean_13b.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/nightlight/vila_train_all_sisv_att_90w_clean_13bV2_our_task.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/nightlight/vila_train_all_sisv_att_186w_clean_13bV2.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/paper_results/pop/vila_train_all_sisv_att_186w_clean_13bV2.jsonl\")\n",
    "evaluation_metrics(\"/mnt/public/vila-1.5/1113_results/pop/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "\n",
    "\n",
    "'''\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/citybench/pop/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")\n",
    "evaluation_metrics(\"/data3/zhangxin/wuwen/vila-1.5/citybench/pop/vila_train_all_sisv_pretrain_att_186w_clean_8b_pop.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/data1/zhangxin/llm/code/LLaVA/result/lrben/llava-v1.5-7b-remoteclip/1_0.jsonl', 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "    \n",
    "with open('/data1/zhangxin/llm/data/Geochat_Data/GeoChat_Bench/all_answers.json', 'r') as file:\n",
    "    all_answers = [json.loads(line) for line in file]\n",
    "    \n",
    "question_id_to_answer = {}\n",
    "for item in all_answers[0][\"answers\"]:\n",
    "    question_id_to_answer[item['question_id']] = item['answer']\n",
    "    \n",
    "new_data = []\n",
    "for item in data:\n",
    "    new_item = item\n",
    "    new_item['ground_truth_ans'] = question_id_to_answer[item['question_id']]\n",
    "    new_data.append(new_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/public/data/FIT-RS/eval/FIT-RSFG_FIT-RSFG-Bench_test_FITRS_complex_comprehension_eval.jsonl\",\"r\") as f:\n",
    "    fitrsrc = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "with open(\"/mnt/public/data/FIT-RS/eval/FIT-RSFG_FIT-RSFG-Bench_test_FITRS_complex_comprehension_eval.jsonl\",\"r\") as f:\n",
    "    fitrsrc = [json.loads(line) for line in f]\n",
    "def obb2poly_np_oc(rbboxes):\n",
    "    \"\"\"Convert oriented bounding boxes to polygons.\n",
    "\n",
    "    Args:\n",
    "        obbs (ndarray): [x_ctr,y_ctr,w,h,angle,score]\n",
    "\n",
    "    Returns:\n",
    "        polys (ndarray): [x0,y0,x1,y1,x2,y2,x3,y3,score]\n",
    "    \"\"\"\n",
    "    x = rbboxes[0]\n",
    "    y = rbboxes[1]\n",
    "    w = rbboxes[2]\n",
    "    h = rbboxes[3]\n",
    "    a = rbboxes[4]\n",
    "    cosa = np.cos(a)\n",
    "    sina = np.sin(a)\n",
    "    wx, wy = w / 2 * cosa, w / 2 * sina\n",
    "    hx, hy = -h / 2 * sina, h / 2 * cosa\n",
    "    p1x, p1y = x - wx - hx, y - wy - hy\n",
    "    p2x, p2y = x + wx - hx, y + wy - hy\n",
    "    p3x, p3y = x + wx + hx, y + wy + hy\n",
    "    p4x, p4y = x - wx + hx, y - wy + hy\n",
    "    polys = np.stack([p1x, p1y, p2x, p2y, p3x, p3y, p4x, p4y])\n",
    "    polys = np.expand_dims(polys, axis=0)\n",
    "    return polys\n",
    "\n",
    "def convert_obb_to_region_str(rbox_np):\n",
    "    angle = rbox_np[-1]\n",
    "    polys = obb2poly_np_oc(rbox_np)\n",
    "    x_left = np.clip(np.min(polys[:, [0, 2, 4, 6]], axis=1), 0, None)\n",
    "    y_top = np.clip(np.min(polys[:, [1, 3, 5, 7]], axis=1), 0, None)\n",
    "    x_right = np.max(polys[:, [0, 2, 4, 6]], axis=1)\n",
    "    y_bottom = np.max(polys[:, [1, 3, 5, 7]], axis=1)\n",
    "    region_str = f\"<{int(x_left[0])}><{int(y_top[0])}><{int(x_right[0])}><{int(y_bottom[0])}>|<{int(angle)}>\"\n",
    "    return region_str\n",
    "\n",
    "for j in range(len(fitrsrc)):\n",
    "    image_file=fitrsrc[j]['image']\n",
    "    # 判断问题类别,进而确定模板\n",
    "    category = fitrsrc[j]['category']\n",
    "    qs = fitrsrc[j]['question']\n",
    "    # 需要的区域任务要针对geochat进行OBB的格式转换\n",
    "    if category in [\"task4\", \"task5\", \"task6\"]:\n",
    "        # if 'fgrs' not in answers_file.split(\"/\")[-1] and 'geochat' in answers_file.split(\"/\")[-1]:\n",
    "        pattern = r'\\{(.+?)\\}'\n",
    "        matches = re.findall(pattern, qs)\n",
    "        for match in matches:\n",
    "            numbers_str = match\n",
    "            pattern = r'<(.+?)>'\n",
    "            numbers = re.findall(pattern, numbers_str)\n",
    "            rbox_np = np.array(numbers, dtype=float)\n",
    "            region_str = convert_obb_to_region_str(rbox_np)\n",
    "            fitrsrc[j]['question'] = qs.replace(numbers_str, region_str)\n",
    "with open(\"/mnt/public/data/FIT-RS/eval/FIT-RSFG_FIT-RSFG-Bench_test_FITRS_complex_comprehension_eval_2ploygon.jsonl\", 'w') as f:\n",
    "    for item in fitrsrc:\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11174/11174 [00:00<00:00, 22154.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======iou thr: 0.25========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-19 09:45:10,989] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-09-19 09:45:10,995] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-09-19 09:45:11,106] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-09-19 09:45:11,106] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------+-----+------+--------+-------+\n",
      "| class | gts | dets | recall | ap    |\n",
      "+-------+-----+------+--------+-------+\n",
      "| 0     | 227 | 316  | 0.233  | 0.053 |\n",
      "| 1     | 36  | 75   | 0.000  | 0.000 |\n",
      "| 2     | 133 | 98   | 0.045  | 0.015 |\n",
      "| 3     | 121 | 27   | 0.025  | 0.013 |\n",
      "| 4     | 116 | 202  | 0.207  | 0.038 |\n",
      "| 5     | 93  | 197  | 0.032  | 0.002 |\n",
      "| 6     | 99  | 189  | 0.111  | 0.022 |\n",
      "| 7     | 729 | 752  | 0.067  | 0.010 |\n",
      "| 8     | 40  | 36   | 0.250  | 0.154 |\n",
      "| 9     | 31  | 24   | 0.000  | 0.000 |\n",
      "| 10    | 74  | 45   | 0.149  | 0.049 |\n",
      "| 11    | 52  | 26   | 0.154  | 0.058 |\n",
      "| 12    | 68  | 52   | 0.118  | 0.061 |\n",
      "| 13    | 10  | 9    | 0.000  | 0.000 |\n",
      "| 14    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 15    | 4   | 5    | 0.000  | 0.000 |\n",
      "| 16    | 67  | 54   | 0.090  | 0.018 |\n",
      "| 17    | 14  | 11   | 0.071  | 0.015 |\n",
      "| 18    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 19    | 3   | 3    | 0.000  | 0.000 |\n",
      "| 20    | 6   | 4    | 0.167  | 0.045 |\n",
      "| 21    | 5   | 5    | 0.000  | 0.000 |\n",
      "| 22    | 1   | 1    | 0.000  | 0.000 |\n",
      "| 23    | 1   | 1    | 0.000  | 0.000 |\n",
      "| 24    | 3   | 2    | 0.000  | 0.000 |\n",
      "| 25    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 26    | 72  | 61   | 0.222  | 0.086 |\n",
      "| 27    | 1   | 1    | 0.000  | 0.000 |\n",
      "| 28    | 4   | 4    | 1.000  | 1.000 |\n",
      "| 29    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 30    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 31    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 32    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 33    | 1   | 1    | 0.000  | 0.000 |\n",
      "| 34    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 35    | 10  | 10   | 0.100  | 0.045 |\n",
      "| 36    | 8   | 10   | 0.000  | 0.000 |\n",
      "| 37    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 38    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 39    | 10  | 10   | 0.700  | 0.525 |\n",
      "| 40    | 11  | 9    | 0.182  | 0.091 |\n",
      "| 41    | 1   | 1    | 1.000  | 1.000 |\n",
      "| 42    | 1   | 1    | 1.000  | 1.000 |\n",
      "| 43    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 44    | 3   | 3    | 1.000  | 1.000 |\n",
      "| 45    | 5   | 3    | 0.000  | 0.000 |\n",
      "| 46    | 1   | 1    | 0.000  | 0.000 |\n",
      "| 47    | 0   | 0    | 0.000  | 0.000 |\n",
      "+-------+-----+------+--------+-------+\n",
      "| mAP   |     |      |        | 0.147 |\n",
      "+-------+-----+------+--------+-------+\n",
      "Task-Object Detection mean ap: 0.14725886285305023\n",
      "Task-Relation Detection Average Precision: 0.6788\n",
      "Task-Relation Detection Average Recall: 0.9705\n",
      "Task-Relation Detection F1 score: 0.7989\n",
      "Task-Relation Reasoning Average Precision: 0.0000\n",
      "Task-Relation Reasoning Average Recall: 0.0000\n",
      "Task-Relation Reasoning F1 score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-19 09:45:18,873] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-09-19 09:45:18,885] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-09-19 09:45:18,903] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-09-19 09:45:18,916] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------+------+------+--------+-------+\n",
      "| class | gts  | dets | recall | ap    |\n",
      "+-------+------+------+--------+-------+\n",
      "| 0     | 188  | 130  | 0.069  | 0.009 |\n",
      "| 1     | 2970 | 2459 | 0.043  | 0.011 |\n",
      "| 2     | 11   | 12   | 0.000  | 0.000 |\n",
      "| 3     | 15   | 7    | 0.000  | 0.000 |\n",
      "| 4     | 52   | 46   | 0.000  | 0.000 |\n",
      "| 5     | 72   | 53   | 0.028  | 0.004 |\n",
      "| 6     | 8    | 8    | 0.125  | 0.023 |\n",
      "| 7     | 409  | 397  | 0.044  | 0.061 |\n",
      "| 8     | 4    | 3    | 0.500  | 0.455 |\n",
      "| 9     | 0    | 0    | 0.000  | 0.000 |\n",
      "| 10    | 1    | 1    | 0.000  | 0.000 |\n",
      "| 11    | 2    | 0    | 0.000  | 0.000 |\n",
      "| 12    | 49   | 13   | 0.000  | 0.000 |\n",
      "| 13    | 2    | 1    | 0.000  | 0.000 |\n",
      "| 14    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 15    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 16    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 17    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 18    | 0    | 1    | 0.000  | 0.000 |\n",
      "| 19    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 20    | 1    | 1    | 0.000  | 0.000 |\n",
      "| 21    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 22    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 23    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 24    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 25    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 26    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 27    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 28    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 29    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 30    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 31    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 32    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 33    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 34    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 35    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 36    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 37    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 38    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 39    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 40    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 41    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 42    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 43    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 44    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 45    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 46    | 0    | 0    | 0.000  | 0.000 |\n",
      "| 47    | 0    | 0    | 0.000  | 0.000 |\n",
      "+-------+------+------+--------+-------+\n",
      "| mAP   |      |      |        | 0.040 |\n",
      "+-------+------+------+--------+-------+\n",
      "Task-Object Reasoning mean ap: 0.0401877835392952\n",
      "Task-Region-level SGG result:\n",
      "result_str:\n",
      "====================================================================================================\n",
      "SGG eval:     R @ 1000: 0.0078;     R @ 1500: 0.0078;     R @ 2000: 0.0078;  for mode=sgdet, type=Recall(Main).\n",
      "SGG eval:    mR @ 1000: 0.0033;    mR @ 1500: 0.0033;    mR @ 2000: 0.0033;  for mode=sgdet, type=Mean Recall.\n",
      "----------------------- Details ------------------------\n",
      "(over:0.0000) (not co-storage with:0.0000) (connect:0.0000) (parallelly parked on:0.0000) (intersect:0.0000) (co-storage with:0.0000) (converge:0.0000) (parallelly docked at:0.0000) (adjacent:0.1429) (within safe distance of:0.0000) (through:0.0000) (approach:0.0000) (away from:0.0000) (randomly parked on:0.0000) (run along:0.0000) (isolatedly parked on:0.0000) (around:0.0000) (randomly docked at:0.0000) (drive off:0.0000) (drive toward:0.0000) (within danger distance of:0.0000) (supply to:0.0000) (isolatedly docked at:0.0000) (pass across:0.0000) (not run along:0.0000) (slightly emit:0.0000) (exhaust to:0.0000) (violently emit:0.0000) (incorrectly parked on:0.0000) (pass under:0.0000) (directly transmit electricity to:0.0000) (indirectly transmit electricity to:0.0000) (pass through:0.0000) (within same line of:0.0000) (within different line of:0.0000) (directly connected to:0.0000) (indirectly connected to:0.0000) (driving in the same direction with:0.0000) (driving in the opposite direction with:0.0000) (driving alongside with:0.0000) (driving in the same lane with:0.0469) (driving in the different lane with:0.0000) (working on:0.0000) (not working on:0.0000) (parked alongside with:0.0000) (not parked alongside with:0.0000) (in the same parking with:0.0000) (in the different parking with:0.0000) (parking in the same apron with:0.0000) (parking in the different apron with:0.0000) (running along the same taxiway with:0.0000) (running along the different taxiway with:0.0000) (running along the different runway with:0.0000) (docking at the same breakwater with:0.0000) (docking at the same dock with:0.0000) (docking at the different dock with:0.0000) (docked alongside with:0.0000) (not docked alongside with:0.0000) \n",
      "--------------------------------------------------------\n",
      "SGG eval:     A @ 1000: nan;     A @ 1500: nan;     A @ 2000: nan;  for mode=sgdet, type=TopK Accuracy.\n",
      "====================================================================================================\n",
      "\n",
      "Task-Image-level SGG result:\n",
      "result_str:\n",
      "====================================================================================================\n",
      "SGG eval:     R @ 1000: 0.0000;     R @ 1500: 0.0000;     R @ 2000: 0.0000;  for mode=sgdet, type=Recall(Main).\n",
      "SGG eval:    mR @ 1000: 0.0000;    mR @ 1500: 0.0000;    mR @ 2000: 0.0000;  for mode=sgdet, type=Mean Recall.\n",
      "----------------------- Details ------------------------\n",
      "(over:0.0000) (not co-storage with:0.0000) (connect:0.0000) (parallelly parked on:0.0000) (intersect:0.0000) (co-storage with:0.0000) (converge:0.0000) (parallelly docked at:0.0000) (adjacent:0.0000) (within safe distance of:0.0000) (through:0.0000) (approach:0.0000) (away from:0.0000) (randomly parked on:0.0000) (run along:0.0000) (isolatedly parked on:0.0000) (around:0.0000) (randomly docked at:0.0000) (drive off:0.0000) (drive toward:0.0000) (within danger distance of:0.0000) (supply to:0.0000) (isolatedly docked at:0.0000) (pass across:0.0000) (not run along:0.0000) (slightly emit:0.0000) (exhaust to:0.0000) (violently emit:0.0000) (incorrectly parked on:0.0000) (pass under:0.0000) (directly transmit electricity to:0.0000) (indirectly transmit electricity to:0.0000) (pass through:0.0000) (within same line of:0.0000) (within different line of:0.0000) (directly connected to:0.0000) (indirectly connected to:0.0000) (driving in the same direction with:0.0000) (driving in the opposite direction with:0.0000) (driving alongside with:0.0000) (driving in the same lane with:0.0000) (driving in the different lane with:0.0000) (working on:0.0000) (not working on:0.0000) (parked alongside with:0.0000) (not parked alongside with:0.0000) (in the same parking with:0.0000) (in the different parking with:0.0000) (parking in the same apron with:0.0000) (parking in the different apron with:0.0000) (running along the same taxiway with:0.0000) (running along the different taxiway with:0.0000) (running along the different runway with:0.0000) (docking at the same breakwater with:0.0000) (docking at the same dock with:0.0000) (docking at the different dock with:0.0000) (docked alongside with:0.0000) (not docked alongside with:0.0000) \n",
      "--------------------------------------------------------\n",
      "SGG eval:     A @ 1000: nan;     A @ 1500: nan;     A @ 2000: nan;  for mode=sgdet, type=TopK Accuracy.\n",
      "====================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-19 09:45:28,179] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-09-19 09:45:28,294] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-09-19 09:45:28,303] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-09-19 09:45:28,318] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/mnt/public/vila/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------+-----+------+--------+-------+\n",
      "| class | gts | dets | recall | ap    |\n",
      "+-------+-----+------+--------+-------+\n",
      "| 0     | 92  | 55   | 0.152  | 0.046 |\n",
      "| 1     | 23  | 6    | 0.043  | 0.045 |\n",
      "| 2     | 122 | 78   | 0.090  | 0.017 |\n",
      "| 3     | 52  | 0    | 0.000  | 0.000 |\n",
      "| 4     | 31  | 8    | 0.032  | 0.023 |\n",
      "| 5     | 31  | 22   | 0.032  | 0.045 |\n",
      "| 6     | 54  | 38   | 0.019  | 0.005 |\n",
      "| 7     | 478 | 183  | 0.013  | 0.005 |\n",
      "| 8     | 6   | 0    | 0.000  | 0.000 |\n",
      "| 9     | 4   | 0    | 0.000  | 0.000 |\n",
      "| 10    | 22  | 11   | 0.091  | 0.045 |\n",
      "| 11    | 36  | 11   | 0.111  | 0.082 |\n",
      "| 12    | 63  | 6    | 0.032  | 0.045 |\n",
      "| 13    | 1   | 0    | 0.000  | 0.000 |\n",
      "| 14    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 15    | 2   | 1    | 0.000  | 0.000 |\n",
      "| 16    | 28  | 4    | 0.000  | 0.000 |\n",
      "| 17    | 8   | 2    | 0.000  | 0.000 |\n",
      "| 18    | 2   | 0    | 0.000  | 0.000 |\n",
      "| 19    | 2   | 0    | 0.000  | 0.000 |\n",
      "| 20    | 1   | 1    | 0.000  | 0.000 |\n",
      "| 21    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 22    | 2   | 1    | 0.500  | 0.545 |\n",
      "| 23    | 1   | 0    | 0.000  | 0.000 |\n",
      "| 24    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 25    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 26    | 64  | 59   | 0.297  | 0.106 |\n",
      "| 27    | 2   | 0    | 0.000  | 0.000 |\n",
      "| 28    | 3   | 1    | 0.333  | 0.364 |\n",
      "| 29    | 1   | 0    | 0.000  | 0.000 |\n",
      "| 30    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 31    | 0   | 1    | 0.000  | 0.000 |\n",
      "| 32    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 33    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 34    | 1   | 0    | 0.000  | 0.000 |\n",
      "| 35    | 12  | 8    | 0.000  | 0.000 |\n",
      "| 36    | 2   | 4    | 0.000  | 0.000 |\n",
      "| 37    | 1   | 0    | 0.000  | 0.000 |\n",
      "| 38    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 39    | 6   | 5    | 0.500  | 0.327 |\n",
      "| 40    | 1   | 0    | 0.000  | 0.000 |\n",
      "| 41    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 42    | 1   | 0    | 0.000  | 0.000 |\n",
      "| 43    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 44    | 20  | 13   | 0.550  | 0.491 |\n",
      "| 45    | 8   | 4    | 0.125  | 0.045 |\n",
      "| 46    | 0   | 0    | 0.000  | 0.000 |\n",
      "| 47    | 0   | 0    | 0.000  | 0.000 |\n",
      "+-------+-----+------+--------+-------+\n",
      "| mAP   |     |      |        | 0.064 |\n",
      "+-------+-----+------+--------+-------+\n",
      "Task-Image-level SGG mean ap: 0.06398671865463257\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sgg_eval import Compute_Pred_Matches\n",
    "from vg_eval import do_vg_evaluation\n",
    "import numpy as np\n",
    "from eval_map import eval_rbbox_map \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ## all categories\n",
    "label_id = ['airplane', 'boat', 'taxiway', 'boarding_bridge', 'tank', 'ship', 'crane',\n",
    "            'car', 'apron', 'dock', 'storehouse', 'goods_yard', 'truck', 'terminal',\n",
    "            'runway', 'breakwater', 'car_parking', 'bridge', 'cooling_tower',\n",
    "            'truck_parking', 'chimney', 'vapor', 'coal_yard', 'genset', 'smoke',\n",
    "            'gas_station', 'lattice_tower', 'substation', 'containment_vessel', 'flood_dam', 'ship_lock', 'gravity_dam',\n",
    "            'arch_dam', 'cement_concrete_pavement', 'toll_gate', 'tower_crane', 'engineering_vehicle', 'unfinished_building', 'foundation_pit',\n",
    "            'wind_mill', 'intersection', 'roundabout', 'ground_track_field', 'soccer_ball_field', 'basketball_court', 'tennis_court', 'baseball_diamond', 'stadium']\n",
    "\n",
    "# ## all relationships\n",
    "relations = ['over', 'not co-storage with', 'connect', 'parallelly parked on', 'intersect', 'co-storage with', 'converge','parallelly docked at', 'adjacent', 'within safe distance of', 'through', 'approach', 'away from', 'randomly parked on', 'run along', 'isolatedly parked on', 'around', 'randomly docked at', 'drive off',\n",
    "             'drive toward', 'within danger distance of','supply to','isolatedly docked at','pass across','not run along','slightly emit','exhaust to','violently emit',\n",
    "             'incorrectly parked on', 'pass under', 'directly transmit electricity to','indirectly transmit electricity to', 'pass through','within same line of', 'within different line of','directly connected to','indirectly connected to','driving in the same direction with',\n",
    "             'driving in the opposite direction with', 'driving alongside with','driving in the same lane with','driving in the different lane with','working on','not working on','parked alongside with','not parked alongside with',\n",
    "             'in the same parking with','in the different parking with','parking in the same apron with','parking in the different apron with','running along the same taxiway with','running along the different taxiway with',\n",
    "             'running along the different runway with','docking at the same breakwater with','docking at the same dock with','docking at the different dock with','docked alongside with','not docked alongside with']\n",
    "\n",
    "label_id_to_index = {label: index for index, label in enumerate(label_id)}\n",
    "relation_to_index = {relation: index for index, relation in enumerate(relations)}\n",
    "\n",
    "# target_id, target_cat, relation, obj_id_count, obj_cat\n",
    "def convert_to_numpy_triplet(sub_id, sub_cat, rel, obj_id, obj_cat):\n",
    "    sub_cat_index = label_id_to_index.get(sub_cat, -1)\n",
    "    rel_index = relation_to_index.get(rel, -1)\n",
    "    obj_cat_index = label_id_to_index.get(obj_cat, -1)\n",
    "    return (sub_id, sub_cat_index, rel_index, obj_id, obj_cat_index)\n",
    "\n",
    "\n",
    "def obb2poly_np_oc(rbboxes):\n",
    "    \"\"\"Convert oriented bounding boxes to polygons.\n",
    "\n",
    "    Args:\n",
    "        obbs (ndarray): [x_ctr,y_ctr,w,h,angle,score]\n",
    "\n",
    "    Returns:\n",
    "        polys (ndarray): [x0,y0,x1,y1,x2,y2,x3,y3,score]\n",
    "    \"\"\"\n",
    "    x = rbboxes[0]\n",
    "    y = rbboxes[1]\n",
    "    w = rbboxes[2]\n",
    "    h = rbboxes[3]\n",
    "    a = rbboxes[4]\n",
    "    cosa = np.cos(a)\n",
    "    sina = np.sin(a)\n",
    "    wx, wy = w / 2 * cosa, w / 2 * sina\n",
    "    hx, hy = -h / 2 * sina, h / 2 * cosa\n",
    "    p1x, p1y = x - wx - hx, y - wy - hy\n",
    "    p2x, p2y = x + wx - hx, y + wy - hy\n",
    "    p3x, p3y = x + wx + hx, y + wy + hy\n",
    "    p4x, p4y = x - wx + hx, y - wy + hy\n",
    "    polys = np.stack([p1x, p1y, p2x, p2y, p3x, p3y, p4x, p4y])\n",
    "    polys = np.expand_dims(polys, axis=0)\n",
    "    return polys\n",
    "\n",
    "\n",
    "# 过滤过小box,否则后续计算会出错\n",
    "def filter_rbox(rbox):\n",
    "    if len(rbox) == 5:\n",
    "        _, _, w, h, _ = rbox\n",
    "    elif len(rbox) == 6:\n",
    "        _, _, w, h, _, _ = rbox\n",
    "    else: # 长度不对\n",
    "        return False\n",
    "    if w < 2 or h < 2:\n",
    "        return False\n",
    "    # elif w < 10 or h <10:\n",
    "    #     rbox[2] = rbox[2]*10\n",
    "    #     rbox[3] = rbox[3]*10 #放大\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def convert_obb_to_region_str(rbox_np):\n",
    "    angle = rbox_np[-1]\n",
    "    polys = obb2poly_np_oc(rbox_np)\n",
    "    x_left = np.clip(np.min(polys[:, [0, 2, 4, 6]], axis=1), 0, None)\n",
    "    y_top = np.clip(np.min(polys[:, [1, 3, 5, 7]], axis=1), 0, None)\n",
    "    x_right = np.max(polys[:, [0, 2, 4, 6]], axis=1)\n",
    "    y_bottom = np.max(polys[:, [1, 3, 5, 7]], axis=1)\n",
    "    region_str = f\"<{int(x_left[0])}><{int(y_top[0])}><{int(x_right[0])}><{int(y_bottom[0])}>|<{int(angle)}>\"\n",
    "    return region_str\n",
    "\n",
    "\n",
    "def extract_rbox_from_str(match, \n",
    "                          pattern = r'<(.*?)>'):\n",
    "    '''\n",
    "    input: <cx><cy><w><h>|<angle> (under 'oc' definition, angle is degree), str '<cx><cy><w><h>|<angle>'\n",
    "    output: (cx, cy, w, h, angle) (angle is rad)\n",
    "    '''\n",
    "    numbers_str = re.findall(pattern, match)\n",
    "    try:\n",
    "        rbox = np.array(numbers_str, dtype=float)\n",
    "    except ValueError:\n",
    "        default_rbox =np.array([0.,0.,0.,0.,0], dtype=float)\n",
    "        rbox = default_rbox\n",
    "    if len(rbox) ==0: #没提取到\n",
    "        return np.array([0.,0.,0.,0.,0], dtype=float)\n",
    "    rbox[-1] = np.deg2rad(rbox[-1])\n",
    "    return rbox\n",
    "\n",
    "def extract_multi_rboxes_from_str(input_str):\n",
    "    # 定义正则表达式模式，用于匹配每个矩形框\n",
    "    pattern = r'\\{(<.*?>)\\}'\n",
    "    # 使用正则表达式找到所有的矩形框\n",
    "    matches = re.findall(pattern, input_str)\n",
    "    rboxes = []\n",
    "    # default_rbox = '({<-3><-3><3><3>|<0>})'\n",
    "    default_rbox =np.array([0.,0.,0.,0.,0], dtype=float)\n",
    "    for match in matches:\n",
    "        # 在每个矩形框中，找到所有的数字\n",
    "        numbers_str = re.findall(r'<(.*?)>', match)\n",
    "        # 将数字转换为浮点数，并将角度转换为弧度\n",
    "        try:\n",
    "            rbox = np.array(numbers_str, dtype=float)\n",
    "        except ValueError:\n",
    "            # 如果转换失败，返回默认的数组\n",
    "            rbox = default_rbox\n",
    "        rbox[-1] = np.deg2rad(rbox[-1])\n",
    "        # if filter_rbox(rbox):\n",
    "        rboxes.append(rbox)\n",
    "    # 将所有的矩形框参数合并成一个 numpy 数组\n",
    "    return np.array(rboxes)\n",
    "\n",
    "\n",
    "### for list convert to numpy for calculate mAP\n",
    "def convert_list_to_rboxeval(det_result_input, annotation_input):\n",
    "    det_results = [[] for _ in range(len(det_result_input))]\n",
    "    num_classes = len(label_id)\n",
    "    annotations = []\n",
    "    # 遍历每个图像的检测结果\n",
    "    for i, image_results in enumerate(det_result_input):\n",
    "        ## 1) 处理annotation_input为要求格式\n",
    "        image_annotations = annotation_input[i]\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        # 遍历这个图像的每个注释\n",
    "        for annotation in image_annotations:\n",
    "            # 将这个注释的bbox和label添加到结果列表中\n",
    "            bboxes.append(annotation['bbox'])\n",
    "            labels.append(annotation['category_id'])\n",
    "        if not bboxes:\n",
    "            continue\n",
    "        bboxes = np.vstack(bboxes)\n",
    "        labels = np.array(labels)\n",
    "        # 将这个图像的bbox和label结果添加到总结果列表中\n",
    "        annotations.append({'bboxes': bboxes, 'labels': labels})\n",
    "        ## 2) 处理det_result_input为要求格式\n",
    "        # 初始化一个列表来保存每个类别的检测结果\n",
    "        per_class_results = [np.zeros((0, 6)) for _ in range(num_classes)]\n",
    "        per_class_tmp_list = [[] for _ in range(num_classes)]\n",
    "        # 遍历这个图像的每个检测结果\n",
    "        for result in image_results:\n",
    "            # 将这个检测结果添加到对应类别的结果列表中\n",
    "            category_id = result['category_id']\n",
    "            per_class_tmp_list[category_id].append(result['bbox'])\n",
    "        # 将每个类别的结果合并为一个 (n, 6) 的数组，并添加到总结果列表中\n",
    "        for j in range(num_classes):\n",
    "            if per_class_tmp_list[j]:\n",
    "                per_class_results[j] = np.vstack(per_class_tmp_list[j])\n",
    "        det_results[i] = per_class_results\n",
    "\n",
    "    det_results = [x for x in det_results if x!=[]]\n",
    "    return det_results, annotations\n",
    "\n",
    "### for task2\n",
    "def calculate_relationships_acc(gt_relationships, pred_relationships):\n",
    "    gt_rels = set(gt_relationships)\n",
    "    pred_rels = set(pred_relationships)\n",
    "    # Calculate the number of true positives (tp), false positives (fp), and false negatives (fn)\n",
    "    tp = len(gt_rels & pred_rels)\n",
    "    fp = len(pred_rels - gt_rels)\n",
    "    fn = len(gt_rels - pred_rels)\n",
    "    # Calculate precision and recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    return precision, recall\n",
    "\n",
    "def calculate_relationships_tpfp(gt_relationships, pred_relationships):\n",
    "    gt_rels = set(gt_relationships)\n",
    "    pred_rels = set(pred_relationships)\n",
    "    # Calculate the number of true positives (tp), false positives (fp), and false negatives (fn)\n",
    "    tp = len(gt_rels & pred_rels)\n",
    "    fp = len(pred_rels - gt_rels)\n",
    "    fn = len(gt_rels - pred_rels)\n",
    "    return tp,fp,fn\n",
    "\n",
    "def calculate_relationships_PRF1(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return precision, recall, f1\n",
    "\n",
    "def parse_single_triplet(triplet_str):\n",
    "    # 使用正则表达式找到三元组的各个部分\n",
    "    region1 = re.findall(r'subject: (.+?),', triplet_str)\n",
    "    region2 = re.findall(r'object: (.+?),', triplet_str)\n",
    "    # 这里是单类别1对1, 还未考虑1对多匹配\n",
    "    relationship = re.findall(r'<rel>(.*?)</rel>', triplet_str)\n",
    "    # 如果任何一个部分的格式不正确，返回 None\n",
    "    if len(region1) == 0 or len(region2) == 0 or len(relationship) == 0:\n",
    "        return [], [], []\n",
    "    \n",
    "    return region1[0], region2[0], relationship\n",
    "\n",
    "def parse_multi_catgory_rbox(input_string, add_score = False):\n",
    "    # 提取所有的目标类别和对应的rbox\n",
    "    pattern = r'<ref>(.*?)</ref><rbox>\\((.*?)\\)</rbox>'\n",
    "    matches = re.findall(pattern, input_string)\n",
    "    categories = []\n",
    "    rboxes = []\n",
    "    for match in matches:\n",
    "        # 提取类别，并转换为对应的label_id\n",
    "        category = match[0]\n",
    "        if category.endswith('s'):\n",
    "            category = category[:-1]\n",
    "        category_id = label_id_to_index.get(category, -1)\n",
    "        categories.append(category_id)\n",
    "        # 提取rbox，并转换为numpy数组\n",
    "        rbox_strs = match[1]\n",
    "        tmp_rboxes = extract_multi_rboxes_from_str(rbox_strs)\n",
    "        num_obj = tmp_rboxes.shape[0]\n",
    "        for i in range(num_obj):\n",
    "            rbox = tmp_rboxes[i]\n",
    "            if add_score:\n",
    "                rbox = np.append(rbox, 1.0)\n",
    "            if filter_rbox(rbox):\n",
    "                rboxes.append(rbox)\n",
    "\n",
    "    if len(rboxes) > 0:\n",
    "        rboxes_categories = list(zip(map(tuple, rboxes), categories))\n",
    "        rboxes_categories = list(dict.fromkeys(rboxes_categories))\n",
    "        rboxes, categories = zip(*rboxes_categories)\n",
    "        rboxes = [np.array(rbox) for rbox in rboxes]\n",
    "\n",
    "    det_result_per_image = [{'bbox': rbox, 'category_id': category_id} for rbox, category_id in zip(rboxes, categories)]\n",
    "    \n",
    "    return det_result_per_image\n",
    "\n",
    "def parse_multi_rbox_nocatgory(input_string, add_score = False):\n",
    "    pattern = r'(\\{.*?\\})'\n",
    "    matches = re.findall(pattern, input_string)\n",
    "    categories = []\n",
    "    rboxes = []\n",
    "    for match in matches:\n",
    "        # 提取目标类别，并转换为对应的label_id\n",
    "        category_id = 1 # 默认值\n",
    "        categories.append(category_id)\n",
    "        # 提取rbox，并转换为numpy数组\n",
    "        rbox = extract_rbox_from_str(match)\n",
    "        if add_score:\n",
    "            rbox = np.append(rbox, 1.0)\n",
    "        if filter_rbox(rbox):\n",
    "            rboxes.append(rbox)\n",
    "    if len(rboxes) > 0:\n",
    "        # 将rboxes和categories合并为一个列表，每个元素是一个元组(rbox, category_id)\n",
    "        rboxes_categories = list(zip(map(tuple, rboxes), categories))\n",
    "        # 使用dict来删除重复的元素并保持原始顺序\n",
    "        rboxes_categories = list(dict.fromkeys(rboxes_categories))\n",
    "        # 分离rboxes和categories\n",
    "        rboxes, categories = zip(*rboxes_categories)\n",
    "        # 将rboxes转换回numpy.ndarray\n",
    "        rboxes = [np.array(rbox) for rbox in rboxes]\n",
    "    ##\n",
    "    det_result_per_image = [{'bbox': rbox, 'category_id': category_id} for rbox, category_id in zip(rboxes, categories)]\n",
    "    return det_result_per_image\n",
    "\n",
    "\n",
    "size = [\"small\", \"medium\", \"large\", \"giant\"]\n",
    "\n",
    "RBOX_START = '<rbox>'\n",
    "RBOX_END = '</rbox>'\n",
    "REF_START = '<ref>'\n",
    "REF_END = '</ref>'\n",
    "REL_START = '<rel>'\n",
    "REL_END = '</rel>'\n",
    "\n",
    "#### for Task5\n",
    "def extract_triplets_from_str(str, if_gt=True):\n",
    "    # 提取指示目标(区域)类别\n",
    "    target_cat=''\n",
    "    target=''\n",
    "    match = re.search(r'(.*) on the .* part of the image', str.split('.')[0])\n",
    "    if match is not None:\n",
    "        target = match.group(1)\n",
    "        for s in size:\n",
    "            if s in target:\n",
    "                match = re.search(s + r' (.*)', target)\n",
    "                if match is None:\n",
    "                    target = ''\n",
    "                else:\n",
    "                    target = match.group(1)\n",
    "                # target_cat = re.search(s + r' (.*)', target).group(1)\n",
    "                break\n",
    "    elif target == '' and if_gt != True: # 对于answer,如果回答中第一句格式不标准,无类别则用gt的类别来代替\n",
    "        #print('first sentence:',str.split('.')[0])\n",
    "        target_cat=if_gt\n",
    "\n",
    "    # 提取关系和其他对象\n",
    "    # relations = re.findall(r'(\\d+)? (.*?) \\((.*?)\\).*?<(.*)>', str)\n",
    "    # 根据句号\".\"进行断句, 逐句提取三元组\n",
    "    sentences = str.replace('\\n', ' ').split('. ')[1:]\n",
    "    triplets = []\n",
    "    bboxes = []\n",
    "    gt_bboxes = np.array((50.,50.,20.,20.,0.))\n",
    "    obj_id_count = 1\n",
    "    target_id = 0\n",
    "\n",
    "    default_rel = 'background'\n",
    "    default_ref = 'background'\n",
    "    default_rbox = '({<0.><0.><0.><0.>|<0>})'\n",
    "    # 在每一句中寻找relation (\"<>\"内的短语)\n",
    "    for sentence in sentences:\n",
    "        if sentence == \"\":\n",
    "            continue\n",
    "        sentence = sentence.lower()\n",
    "        relation = re.findall(r'<rel>(.*?)</rel>', sentence)\n",
    "        obj_cat = re.findall(r'<ref>(.*?)</ref>', sentence)\n",
    "        unknow_boxes_str = re.findall(r'<rbox>(.*?)</rbox>', sentence)\n",
    "\n",
    "        relation = next((item for item in re.findall(r'<rel>(.*?)</rel>', sentence)), default_rel)\n",
    "        obj_cat = next((item for item in re.findall(r'<ref>(.*?)</ref>', sentence)), default_ref)\n",
    "        unknow_boxes_str = next((item for item in re.findall(r'<rbox>(.*?)</rbox>', sentence)), default_rbox)\n",
    "\n",
    "        rboxes_ = extract_multi_rboxes_from_str(unknow_boxes_str)\n",
    "        count = int(rboxes_.shape[0])\n",
    "\n",
    "        if \"it is\" in sentence:  # it-<rel>-obj\n",
    "            # 考虑复数\n",
    "            if count > 1 and obj_cat.endswith('s'):\n",
    "                obj_cat = obj_cat[:-1]\n",
    "            obj_rboxes = rboxes_\n",
    "            for i in range(count):\n",
    "                if filter_rbox(obj_rboxes[i]):\n",
    "                    triplets.append(convert_to_numpy_triplet(target_id, target_cat, relation, obj_id_count, obj_cat))\n",
    "                    bboxes.append((gt_bboxes, obj_rboxes[i]))\n",
    "                    obj_id_count += 1\n",
    "        elif \"> it\" in sentence: # subj-<rel>-it\n",
    "            if count > 1 and obj_cat.endswith('s'):\n",
    "                obj_cat = obj_cat[:-1]\n",
    "            obj_rboxes = rboxes_\n",
    "            for i in range(count):\n",
    "                if filter_rbox(obj_rboxes[i]):\n",
    "                    triplets.append(convert_to_numpy_triplet(obj_id_count, obj_cat, relation, target_id, target_cat))\n",
    "                    bboxes.append((obj_rboxes[i], gt_bboxes))\n",
    "                    obj_id_count += 1\n",
    "\n",
    "    if if_gt==True:            \n",
    "        return triplets, bboxes, target_cat\n",
    "    else:\n",
    "        return triplets, bboxes\n",
    "\n",
    "#### for Task6\n",
    "def extract_triplets_from_str_task6(str, add_score = False):\n",
    "    \n",
    "    sentences = str.replace('\\n', ' ').split('. ')\n",
    "    triplets = []\n",
    "    bboxes = []\n",
    "    # det_results_per_image = [] \n",
    "    rboxes_score = []\n",
    "    categories = []\n",
    "    id_count = 0 \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        if \"sorry\" in sentence and add_score == False:  # gt为负样本\n",
    "            continue\n",
    "        # Find all <rel> tags\n",
    "        relation = re.findall(r'<rel>(.*?)</rel>', sentence)\n",
    "        ## 1) SGG\n",
    "        if relation:  \n",
    "            relation = relation[0]\n",
    "            ref_values = re.findall(r'<ref>(.*?)</ref>', sentence)\n",
    "            rbox_values = re.findall(r'<rbox>(.*?)</rbox>', sentence)\n",
    "            \n",
    "            default_ref = 'background'  # 考虑错误情况\n",
    "            default_rbox = '({<0.><0.><0.><0.>|<0>})'  # 考虑错误情况\n",
    "            \n",
    "            if len(ref_values)>2:\n",
    "                ref_values=ref_values[:2]\n",
    "                rbox_values=rbox_values[:2]\n",
    "            \n",
    "            while len(ref_values) < 2:\n",
    "                ref_values.append(default_ref)\n",
    "            subj_cat, obj_cat = ref_values\n",
    "            \n",
    "            while len(rbox_values) < 2:\n",
    "                rbox_values.append(default_rbox)\n",
    "            subj_boxes_str, obj_boxes_str = rbox_values\n",
    "\n",
    "            # 考虑复数\n",
    "            if subj_cat.endswith('s'):\n",
    "                subj_cat = subj_cat[:-1]\n",
    "            if obj_cat.endswith('s'):\n",
    "                obj_cat = obj_cat[:-1]\n",
    "            subj_rboxes = extract_multi_rboxes_from_str(subj_boxes_str)\n",
    "            obj_rboxes = extract_multi_rboxes_from_str(obj_boxes_str)\n",
    "            num_subj = subj_rboxes.shape[0]\n",
    "            if obj_rboxes.shape[0] == 0:\n",
    "                continue\n",
    "            assert obj_rboxes.shape[0] <=1\n",
    "            obj_rboxes = obj_rboxes[0]\n",
    "            if not filter_rbox(obj_rboxes):\n",
    "                continue\n",
    "\n",
    "            for i in range(num_subj):\n",
    "                if filter_rbox(subj_rboxes[i]):\n",
    "                    triplets.append(convert_to_numpy_triplet(id_count, subj_cat, relation, id_count+1, obj_cat))\n",
    "                    bboxes.append((subj_rboxes[i], obj_rboxes))  # 这里注意形状要是一维数组\n",
    "                    id_count += 2\n",
    "        \n",
    "        ## 2) Object Detection\n",
    "        elif not relation and RBOX_START in sentence:\n",
    "            default_ref = 'background'\n",
    "            default_rbox = '({<0.><0.><0.><0.>|<0>})'\n",
    "            category = next((item for item in re.findall(r'<ref>(.*?)</ref>', sentence)), default_ref)\n",
    "            rboxes_str = next((item for item in re.findall(r'<rbox>(.*?)</rbox>', sentence)), default_rbox)\n",
    "\n",
    "            # 1) extract category\n",
    "            if category.endswith('s'):\n",
    "                category = category[:-1]\n",
    "            # 2) extract rboxes in ground truth and answer\n",
    "            rboxes = extract_multi_rboxes_from_str(rboxes_str)\n",
    "            num_obj = rboxes.shape[0]\n",
    "            for i in range(num_obj):\n",
    "                rbox = rboxes[i]\n",
    "                if add_score:\n",
    "                    rbox = np.append(rbox, 1.0)\n",
    "                if filter_rbox(rbox):\n",
    "                    # 添加得分\n",
    "                    rboxes_score.append(rbox)\n",
    "                    # categories.append(label_id.index(category))\n",
    "                    categories.append(label_id_to_index.get(category, -1))\n",
    "            # det_result_per_image = [{'bbox': rbox, 'category_id': label_id.index(category)} for rbox in rboxes_score]\n",
    "    \n",
    "    det_results_per_image = [{'bbox': rbox, 'category_id': category_id} for rbox, category_id in zip(rboxes_score, categories)]\n",
    "    \n",
    "    return triplets, bboxes, det_results_per_image\n",
    "\n",
    "\n",
    "def evaluation_metrics_ComplexCompre(data_path):\n",
    "\n",
    "    base = [json.loads(q) for q in open(data_path, \"r\")]\n",
    "    ######## pre definition #########\n",
    "    ## Task1 Object Detection\n",
    "    det_results_task1 = [[] for _ in range(len(base))]\n",
    "    gt_annotations_task1 = [[] for _ in range(len(base))]\n",
    "    ## Task2 Relation Detection\n",
    "    count_task2 = 0\n",
    "    recall_task2 = 0\n",
    "    precision_task2 = 0\n",
    "    tp_task2 = 0\n",
    "    fp_task2 = 0\n",
    "    fn_task2 = 0\n",
    "    ## Task3 Relation Reasoning\n",
    "    recall_task3 = 0\n",
    "    tp_task3 = 0\n",
    "    fp_task3 = 0\n",
    "    fn_task3 = 0\n",
    "    ## Task4 Object Reasoning\n",
    "    det_results_task4 = [[] for _ in range(len(base))]\n",
    "    gt_annotations_task4 = [[] for _ in range(len(base))]\n",
    "    ## Task5 Region Grounding\n",
    "    gt_inputs_task5 = []\n",
    "    predictions_task5 = []\n",
    "    ## Task6 Image Grounding\n",
    "    gt_inputs_task6 = []\n",
    "    predictions_task6 = []\n",
    "    det_results_task6 = [[] for _ in range(len(base))]\n",
    "    gt_annotations_task6 = [[] for _ in range(len(base))]\n",
    "\n",
    "    ################################\n",
    "    # for answers in tqdm(base):\n",
    "    for i, answers in enumerate(tqdm(base)):\n",
    "        # image_id = answers['image_id']\n",
    "        gt = answers['ground_truth']\n",
    "        answer = answers['answer']\n",
    "        task_category = answers['category']\n",
    "        \n",
    "        if \"due to the context length\" in gt or \"...\" in gt:  # NOTE: too long to evaluate, \"...\"则是出现在grounding任务中\n",
    "            continue\n",
    "        pattern_loc = r'\\{(.+?)\\}'\n",
    "        pattern_ = r'<(.+?)>'\n",
    "        if task_category == \"task1\": # Object Detection## Metrics: mAP for all, mean IoU\n",
    "            # 1) extract category\n",
    "            category_match = re.search(r'There (?:is|are) \\d+ (.+?)s? in the image', gt)\n",
    "            if category_match is None:  # 负样本\n",
    "                continue\n",
    "            category = category_match.group(1)\n",
    "            category = category.rstrip('s')\n",
    "            # 2) extract rboxes in ground truth and answer\n",
    "            rbox_matches_gt = re.findall(pattern_loc, gt)\n",
    "            rboxes_gt = []\n",
    "            for match in rbox_matches_gt:\n",
    "                rbox = extract_rbox_from_str(match)\n",
    "                if filter_rbox(rbox):\n",
    "                    rboxes_gt.append(rbox)\n",
    "            rbox_matches_pre = re.findall(pattern_loc, answer)\n",
    "            rboxes_pre = []\n",
    "            for match in rbox_matches_pre:\n",
    "                rbox = extract_rbox_from_str(match)\n",
    "                if filter_rbox(rbox):\n",
    "                    rbox = np.append(rbox, 1.0)\n",
    "                    rboxes_pre.append(rbox)\n",
    "            # 3) append to det_results and gt_annotations\n",
    "            det_results_per_image1 = [{'bbox': rbox, 'category_id': label_id_to_index.get(category, -1)} for rbox in rboxes_pre]\n",
    "            det_results_task1[i].extend(det_results_per_image1)\n",
    "            gt_annotations_per_image1 = [{'bbox': rbox, 'category_id': label_id_to_index.get(category, -1)} for rbox in rboxes_gt]\n",
    "\n",
    "            gt_annotations_task1[i].extend(gt_annotations_per_image1)\n",
    "            continue\n",
    "\n",
    "        elif task_category == \"task2\": # Relationship Detection\n",
    "            # \"ground_truth\": \"There are 2 relationships between tank and tank: tank <not co-storage with> tank, tank <co-storage with> tank\"\n",
    "            # Metrics: Recall, Precision\n",
    "            pattern_r = re.compile(r'<(.*?)>')\n",
    "            rel_gt = re.findall(pattern_r, gt)\n",
    "            rel_pre = re.findall(pattern_r, answer)\n",
    "            tp,fp,fn = calculate_relationships_tpfp(rel_gt, rel_pre)\n",
    "            tp_task2 +=tp\n",
    "            fp_task2 +=fp\n",
    "            fn_task2 +=fn\n",
    "            continue\n",
    "\n",
    "        elif task_category == \"task3\": # Referring Relationship Reasoning\n",
    "            cat1_gt, cat2_gt, rel_gt = parse_single_triplet(gt)\n",
    "            cat1_pre, cat2_pre, rel_pre = parse_single_triplet(answer)\n",
    "            if not rel_gt:  # 负样本\n",
    "                continue\n",
    "            # calculate accuracy\n",
    "            # acc为单标签分类,用于多标签时不会考虑顺序\n",
    "            if cat1_gt == cat1_pre and cat2_gt == cat2_pre:\n",
    "                tp,fp,fn = calculate_relationships_tpfp(rel_gt, rel_pre)\n",
    "                tp_task3 +=tp\n",
    "                fp_task3 +=fp\n",
    "                fn_task3 +=fn\n",
    "            elif cat1_pre!=[] and cat2_pre!=[]: # 类别预测错误\n",
    "                tp = 0\n",
    "                fp = len(rel_pre)\n",
    "                fn = len(rel_gt)\n",
    "            else:  # 类别预测为空\n",
    "                tp = 0\n",
    "                fp = 0\n",
    "                fn = len(rel_gt)\n",
    "            continue\n",
    "\n",
    "        elif task_category == \"task4\": # Object Reasoning\n",
    "            if 'categories' in gt:  # 类别+box\n",
    "                det_results_per_image4 = parse_multi_catgory_rbox(answer, add_score=True)\n",
    "                gt_annotations_per_image4 = parse_multi_catgory_rbox(gt)\n",
    "            else:  # 仅box\n",
    "                det_results_per_image4 = parse_multi_rbox_nocatgory(answer, add_score=True)\n",
    "                gt_annotations_per_image4 = parse_multi_rbox_nocatgory(gt)\n",
    "            det_results_task4[i].extend(det_results_per_image4)\n",
    "            gt_annotations_task4[i].extend(gt_annotations_per_image4)\n",
    "            continue\n",
    "\n",
    "        elif task_category == \"task5\": #  Region Grounding\n",
    "            obj_gt = re.findall(pattern_loc, gt)\n",
    "            if not obj_gt:  # gt不含rbox tag, 无法计算三元组\n",
    "                continue\n",
    "            # obj_pre = re.findall(pattern_loc, answer)\n",
    "            ## 1) 首先从gt和prediction分别提取三元组、关系\n",
    "            # 提取目标对象并保存提及的三元组\n",
    "            gt_triplets, gt_bboxes, target_cat = extract_triplets_from_str(gt, if_gt=True)\n",
    "            pre_triplets, pre_bboxes = extract_triplets_from_str(answer, if_gt=target_cat)\n",
    "            ## 2) 按照SGG中的eval方式来进行评估\n",
    "            # Compute_Pred_Matches(gt_triplets, pre_triplets, gt_bboxes, pre_bboxes, iou_thres=0.5, phrdet=False)\n",
    "            gt_input = {'gt_triplet':gt_triplets, 'gt_bboxes': gt_bboxes}\n",
    "            prediction = {'pred_triplet':pre_triplets, 'pred_bboxes':pre_bboxes}\n",
    "            gt_inputs_task5.append(gt_input)\n",
    "            predictions_task5.append(prediction)\n",
    "            continue\n",
    "\n",
    "        elif task_category == \"task6\": #  Image Grounding\n",
    "            obj_gt = re.findall(pattern_loc, gt)\n",
    "            if not obj_gt:  # gt不含grounding标签, 无法计算三元组\n",
    "                continue\n",
    "            if 'sorry' in gt:  # negative sample\n",
    "                continue\n",
    "            gt_triplets_t6, gt_bboxes_t6, gt_annotations_per_image6 = extract_triplets_from_str_task6(gt)\n",
    "            pre_triplets_t6, pre_bboxes_t6, det_results_per_image6 = extract_triplets_from_str_task6(answer, add_score=True)\n",
    "\n",
    "            ## 2) 按照SGG中的eval方式来进行评估\n",
    "            # Compute_Pred_Matches(gt_triplets, pre_triplets, gt_bboxes, pre_bboxes, iou_thres=0.5, phrdet=False)\n",
    "            gt_input_t6 = {'gt_triplet':gt_triplets_t6, 'gt_bboxes': gt_bboxes_t6}\n",
    "            prediction_t6 = {'pred_triplet':pre_triplets_t6, 'pred_bboxes':pre_bboxes_t6}\n",
    "            gt_inputs_task6.append(gt_input_t6)\n",
    "            predictions_task6.append(prediction_t6)\n",
    "\n",
    "            ## 目标检测评估\n",
    "            gt_annotations_task6[i].extend(gt_annotations_per_image6)\n",
    "            det_results_task6[i].extend(det_results_per_image6)\n",
    "\n",
    "    ######## Output Results #######\n",
    "    iou_thr = 0.25\n",
    "    print(f\"=======iou thr: {iou_thr}========\")\n",
    "    ### Task1\n",
    "    # convert format\n",
    "    det_task_1, gt_task_1 = convert_list_to_rboxeval(det_results_task1, gt_annotations_task1)\n",
    "    # eval map\n",
    "    mean_ap_1, result_1 = eval_rbbox_map(det_task_1, gt_task_1, iou_thr=iou_thr)\n",
    "    print(f\"Task-Object Detection mean ap: {mean_ap_1}\")\n",
    "    ## Task 2\n",
    "    # 新方式\n",
    "    precision_task2, recall_task2, f1_task2 = calculate_relationships_PRF1(tp_task2, fp_task2, fn_task2)\n",
    "    print(f'Task-Relation Detection Average Precision: {precision_task2:.4f}')\n",
    "    print(f'Task-Relation Detection Average Recall: {recall_task2:.4f}')\n",
    "    print(f'Task-Relation Detection F1 score: {f1_task2:.4f}')\n",
    "\n",
    "    ### Task 3\n",
    "    precision_task3, recall_task3, f1_task3 = calculate_relationships_PRF1(tp_task3, fp_task3, fn_task3)\n",
    "    print(f'Task-Relation Reasoning Average Precision: {precision_task3:.4f}')\n",
    "    print(f'Task-Relation Reasoning Average Recall: {recall_task3:.4f}')\n",
    "    print(f'Task-Relation Reasoning F1 score: {f1_task3:.4f}')\n",
    "\n",
    "    ### Task 4\n",
    "    det_task_4, gt_task_4 = convert_list_to_rboxeval(det_results_task4, gt_annotations_task4)\n",
    "    # eval map\n",
    "    mean_ap_4, result_4 = eval_rbbox_map(det_task_4, gt_task_4, iou_thr=iou_thr)\n",
    "    print(f\"Task-Object Reasoning mean ap: {mean_ap_4}\")\n",
    "    ### Task 5\n",
    "    print(\"Task-Region-level SGG result:\")\n",
    "    do_vg_evaluation(gt_inputs_task5, predictions_task5, iou_thres=[iou_thr])\n",
    "    ## Task 6\n",
    "    print(\"Task-Image-level SGG result:\")\n",
    "    do_vg_evaluation(gt_inputs_task6, predictions_task6, iou_thres=[iou_thr])\n",
    "    det_task_6, gt_task_6 = convert_list_to_rboxeval(det_results_task6, gt_annotations_task6)\n",
    "    mean_ap_6, _ = eval_rbbox_map(det_task_6, gt_task_6, iou_thr=iou_thr)\n",
    "    print(f\"Task-Image-level SGG mean ap: {mean_ap_6}\")\n",
    "\n",
    "\n",
    "\n",
    "evaluation_metrics_ComplexCompre(\"/mnt/public/vila-1.5/our_results/com/vila_train_sisv_pretrain_35w_att.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[reasoning]There are some cars that are <rel>driving in the same direction with</rel> the object present at <rbox>({<27><39><35><55>|<63>})</rbox>. Could you tell me their locations?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitrsrc = [json.loads(line) for line in open(\"/mnt/public/vila-1.5/paper_results/spatial/vila_train_all_sisv_pretrain_att_186w_clean.jsonl\")]\n",
    "from collections import defaultdict\n",
    "diff_type_score = defaultdict(list)\n",
    "for item in fitrsrc:\n",
    "    if item['text'] == item['ground_truth']:\n",
    "        diff_type_score[item['category']].append(1)\n",
    "    else:\n",
    "        diff_type_score[item['category']].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relationship: 34.92932056543548\n",
      "exist: 65.0\n",
      "object: 84.55083909180652\n",
      "subject: 48.3998030526834\n"
     ]
    }
   ],
   "source": [
    "for type, type_score in diff_type_score.items():\n",
    "    print(f\"{type}: {100.0 * (sum(type_score) / len(type_score))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10004 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/data1/zhangxin/llm/code/LLaVA/our_eval/test_eval.ipynb 单元格 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c4d31227d/data1/zhangxin/llm/code/LLaVA/our_eval/test_eval.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m evaluator \u001b[39m=\u001b[39m TextVQAAccuracyEvaluator()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c4d31227d/data1/zhangxin/llm/code/LLaVA/our_eval/test_eval.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m result \u001b[39m=\u001b[39m evaluator\u001b[39m.\u001b[39;49meval_pred_list(data)\n",
      "\u001b[1;32m/data1/zhangxin/llm/code/LLaVA/our_eval/test_eval.ipynb 单元格 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c4d31227d/data1/zhangxin/llm/code/LLaVA/our_eval/test_eval.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=239'>240</a>\u001b[0m diff_type_score \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c4d31227d/data1/zhangxin/llm/code/LLaVA/our_eval/test_eval.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=240'>241</a>\u001b[0m \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m tqdm(pred_list):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c4d31227d/data1/zhangxin/llm/code/LLaVA/our_eval/test_eval.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=241'>242</a>\u001b[0m     pred_answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manswer_processor(entry[\u001b[39m\"\u001b[39;49m\u001b[39mpred\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c4d31227d/data1/zhangxin/llm/code/LLaVA/our_eval/test_eval.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=242'>243</a>\u001b[0m     unique_answer_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_answer_scores(entry[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c4d31227d/data1/zhangxin/llm/code/LLaVA/our_eval/test_eval.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=243'>244</a>\u001b[0m     score \u001b[39m=\u001b[39m unique_answer_scores\u001b[39m.\u001b[39mget(pred_answer, \u001b[39m0.0\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pred'"
     ]
    }
   ],
   "source": [
    "evaluator = TextVQAAccuracyEvaluator()\n",
    "result = evaluator.eval_pred_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import bisect\n",
    "def calculate_sum(j, a, prefix_sum, m, n):\n",
    "    # Find the number of students within [j - n/2, j + n/2]\n",
    "    # Since it's a circle, handle wrap-around by duplicating the list\n",
    "    half_n = n / 2\n",
    "    # To handle integer positions, use floor and ceil appropriately\n",
    "    left = j - half_n\n",
    "    right = j + half_n\n",
    "    if left < 0:\n",
    "        left += n\n",
    "    if right >= n:\n",
    "        right -= n\n",
    "    # Find the number of a_k within [left, right]\n",
    "    if left <= right:\n",
    "        cnt = bisect.bisect_right(a, right) - bisect.bisect_left(a, left)\n",
    "        sum_inside = prefix_sum[bisect.bisect_right(a, right)] - prefix_sum[bisect.bisect_left(a, left)]\n",
    "    else:\n",
    "        # Interval wraps around\n",
    "        cnt1 = bisect.bisect_right(a, right)\n",
    "        cnt2 = m - bisect.bisect_left(a, left)\n",
    "        cnt = cnt1 + cnt2\n",
    "        sum_inside = (prefix_sum[bisect.bisect_right(a, right)] - prefix_sum[0]) + \\\n",
    "                     (prefix_sum[m] - prefix_sum[bisect.bisect_left(a, left)])\n",
    "    # Sum of distances inside\n",
    "    # For sum |j - a_k| where a_k within [left, right]\n",
    "    # Since a is sorted, split into two parts: a_k <= j and a_k > j\n",
    "    idx = bisect.bisect_right(a, j)\n",
    "    sum_left = prefix_sum[idx] - prefix_sum[0]\n",
    "    sum_right = prefix_sum[bisect.bisect_right(a, right)] - prefix_sum[idx]\n",
    "    sum_distance_inside = (j * idx - sum_left) + (sum_right - j * (cnt - idx))\n",
    "    # Sum of distances outside\n",
    "    cnt_outside = m - cnt\n",
    "    sum_distance_outside = cnt_outside * n - (prefix_sum[m] - prefix_sum[bisect.bisect_right(a, right)] + prefix_sum[bisect.bisect_left(a, left)])\n",
    "    # Total sum\n",
    "    total = sum_distance_inside + sum_distance_outside\n",
    "    return total\n",
    "\n",
    "def main():\n",
    "    import sys\n",
    "    n, m = map(int,input().split())\n",
    "    \n",
    "    a=list(map(int,input().split()))\n",
    "    a.sort()\n",
    "    prefix_sum = [0] * (m + 1)\n",
    "    for i in range(m):\n",
    "        prefix_sum[i + 1] = prefix_sum[i] + a[i]\n",
    "    # Ternary search over the possible j\n",
    "    left = 1\n",
    "    right = n\n",
    "    def f(j):\n",
    "        # Compute the sum of min distances for position j\n",
    "        total = 0\n",
    "        for pos in a:\n",
    "            d = abs(j - pos)\n",
    "            d = min(d, n - d)\n",
    "            total += d\n",
    "        return total\n",
    "    # Since n can be up to 1e9, use integer ternary search\n",
    "    while right - left > 3:\n",
    "        m1 = left + (right - left) // 3\n",
    "        m2 = right - (right - left) // 3\n",
    "        f1 = f(m1)\n",
    "        f2 = f(m2)\n",
    "        if f1 < f2:\n",
    "            right = m2\n",
    "        else:\n",
    "            left = m1\n",
    "    # After narrowing down, find the minimum in the remaining range\n",
    "    min_sum = float('inf')\n",
    "    best_j = -1\n",
    "    for j in range(left, right + 1):\n",
    "        current_sum = f(j)\n",
    "        if current_sum < min_sum or (current_sum == min_sum and j < best_j):\n",
    "            min_sum = current_sum\n",
    "            best_j = j\n",
    "    print(best_j)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"/mnt/public/ALL_DATA_MIXED_NEW/Socioeconomic_median_income_formatted/Socioeconomic_median_income_data_test_1000.json\",\"r\") as f:\n",
    "    temp = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_id': '6037651101',\n",
       " 'sub_task': 'median income',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': \"<image><image><image><image><image><image>\\nThe images you're viewing are satellite and street views of a census tract in the U.S. How would you rate the median income for this area from 0 to 9.9, with 9.9 being the maximum? Just provide the number. Example format: 'X.X'.\"},\n",
       "  {'from': 'gpt', 'value': '8.1'}],\n",
       " 'image': ['/data3/ouyangtianjian/matched_rs_images/13110_5612.png',\n",
       "  '/data2/ouyangtianjian/US_StreetView_2500_to_5000_CUT/6037651101/2_8CDE9oWZkxrDS23ohG70Ug&33.81489799948314&-118.3428785979579&15.jpg',\n",
       "  '/data2/ouyangtianjian/US_StreetView_2500_to_5000_CUT/6037651101/3_10rFzXlIT9cl0foF9kbHgg&33.81146932602454&-118.3431216043606&15.jpg',\n",
       "  '/data2/ouyangtianjian/NEW_StreetView_Images_US_CUT_merged/US_StreetView_2500_to_5000_CUT/6037651101/h6QI5BH79TRGNBMmmdB1yw&33.81213902077876&-118.3402411757071&15&0.jpg',\n",
       "  '/data2/ouyangtianjian/NEW_StreetView_Images_US_CUT_merged/US_StreetView_2500_to_5000_CUT/6037651101/_wn9g6SzLeT2AV3m5Y3KYQ&33.80757138150287&-118.3339204692409&15&0.jpg',\n",
       "  '/data2/ouyangtianjian/NEW_StreetView_Images_US_CUT_merged/US_StreetView_2500_to_5000_CUT/6037651101/LxOrqtHhIo51m7Yi2K4YGQ&33.81371556131037&-118.340171705939&15&4.jpg'],\n",
       " 'choice_list': 'null',\n",
       " 'metadata': {'dataset': 'Socioeconomic', 'question_type': 'open-ended'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
